import { GoogleGenerativeAI } from "@google/generative-ai";
import { generateNanoImage } from "./imageGenService";
import { stitchVideos } from "./videoProcessingService";
import { AdvancedVideoRequest } from "../types/netflow";
import { getApiKey } from "./storageService";

// Wrapper to get client instance dynamically
const getGenAI = async () => {
    const key = await getApiKey();
    if (!key) throw new Error("API Key not found. Please set it in Settings.");
    return new GoogleGenerativeAI(key);
};

// Extended interface for script generation input - includes ALL form fields
export interface ScriptRequest {
    // Product Info
    productName: string;
    productDescription?: string;
    productId?: string;
    mustUseKeywords?: string;
    avoidKeywords?: string;

    // Script Settings
    style: string;          // saleStyle: hard, soft, educational, storytelling
    tone: string;           // voiceTone: energetic, calm, friendly, professional
    language: string;       // th-central, th-north, th-south, th-isan, en
    template?: string;      // product-review, brainrot-product, etc.
    hookText?: string;      // Opening hook
    ctaText?: string;       // Call to action

    // Character Settings
    gender?: string;        // male, female, any
    ageRange?: string;      // teen, young-adult, adult, etc.
    personality?: string;   // cheerful, calm, professional, playful, mysterious
    background?: string;    // studio, outdoor, home, office, abstract

    // Video Settings
    expression?: string;    // happy, excited, neutral, serious
    movement?: string;      // static, minimal, active
    aspectRatio?: string;   // 9:16, 16:9, 1:1
    videoDuration?: string; // short, medium, long
}

// Interface for the result
export interface ScriptResult {
    script: string;
    audioUrl?: string;
    videoUrl?: string;
}

/**
 * Build a comprehensive prompt from all form fields
 */
const buildFullPrompt = (data: ScriptRequest): string => {
    const templateDescriptions: Record<string, string> = {
        "product-review": "Product review format with honest opinions and recommendations",
        "brainrot-product": "Viral brainrot style with fast cuts and memes mixed with product promotion",
        "unboxing": "Unboxing experience showing first impressions",
        "comparison": "Before/after or vs competitor comparison",
        "testimonial": "Customer testimonial style",
        "flash-sale": "Urgency-driven flash sale promotion",
        "tutorial": "How-to tutorial teaching product usage",
        "lifestyle": "Lifestyle integration showing product in daily life",
        "trending": "Following current TikTok trends",
        "mini-drama": "Short drama/storytelling format",
        "before-after": "Transformation before and after using product"
    };

    const toneDescriptions: Record<string, string> = {
        "energetic": "High energy, enthusiastic, exciting",
        "calm": "Relaxed, soothing, trustworthy",
        "friendly": "Warm, conversational, relatable",
        "professional": "Expert, authoritative, credible"
    };

    const styleDescriptions: Record<string, string> = {
        "hard": "aggressive hard-sell with strong urgency",
        "soft": "gentle persuasion with subtle nudges",
        "educational": "informative and teaching-focused",
        "storytelling": "narrative-driven emotional connection"
    };

    let prompt = `
You are an expert TikTok script writer specializing in viral product videos for Thai audiences.

## PRODUCT INFORMATION
- Product Name: ${data.productName}
${data.productDescription ? `- Description: ${data.productDescription}` : ""}
${data.mustUseKeywords ? `- Must Include Keywords: ${data.mustUseKeywords}` : ""}
${data.avoidKeywords ? `- Avoid These Words: ${data.avoidKeywords}` : ""}

## SCRIPT STYLE
- Template: ${data.template || "product-review"} (${templateDescriptions[data.template || "product-review"] || ""})
- Sales Approach: ${data.style} (${styleDescriptions[data.style] || ""})
- Voice Tone: ${data.tone} (${toneDescriptions[data.tone] || ""})
- Language: ${data.language === "th-central" ? "Thai (Central dialect)" :
            data.language === "th-north" ? "Thai (Northern dialect)" :
                data.language === "th-south" ? "Thai (Southern dialect)" :
                    data.language === "th-isan" ? "Thai (Isan/Northeastern dialect)" : "English"}

## PRESENTER CHARACTER
- Gender: ${data.gender || "female"}
- Age Range: ${data.ageRange || "young-adult"}
- Personality: ${data.personality || "cheerful"}
- Expression: ${data.expression || "happy"}

## VIDEO SPECIFICATIONS
- Background Setting: ${data.background || "studio"}
- Camera Movement: ${data.movement || "minimal"}
- Aspect Ratio: ${data.aspectRatio || "9:16"} (${data.aspectRatio === "9:16" ? "TikTok vertical" : data.aspectRatio === "16:9" ? "YouTube horizontal" : "Instagram square"})
- Duration: ${data.videoDuration === "short" ? "15-30 seconds" : data.videoDuration === "medium" ? "30-60 seconds" : "1-3 minutes"}

## SCRIPT STRUCTURE
${data.hookText ? `- Opening Hook: "${data.hookText}"` : "- Create an attention-grabbing hook"}
${data.ctaText ? `- Call to Action: "${data.ctaText}"` : "- Include a compelling call to action"}

## OUTPUT REQUIREMENTS
Generate a complete TikTok script in ${data.language?.startsWith("th") ? "Thai" : "English"} with:
1. [HOOK] - Attention-grabbing opening (2-3 seconds)
2. [PROBLEM] - Relate to audience pain point
3. [SOLUTION] - Introduce the product as the answer
4. [PROOF] - Benefits, features, or testimonial
5. [CTA] - Clear call to action

Output ONLY the script dialogue. No metadata, timestamps, or stage directions.
Make it sound natural, conversational, and viral-worthy.
`;

    return prompt;
};

/**
 * Helper to generate using OpenAI with full form data
 */
const generateWithOpenAI = async (apiKey: string, data: ScriptRequest): Promise<string> => {
    console.log("ü§ñ Generating script with OpenAI (GPT-4o-mini)...");
    const prompt = buildFullPrompt(data);

    const response = await fetch("https://api.openai.com/v1/chat/completions", {
        method: "POST",
        headers: {
            "Content-Type": "application/json",
            "Authorization": `Bearer ${apiKey}`
        },
        body: JSON.stringify({
            model: "gpt-4o-mini", // Cost efficient and fast
            messages: [
                { role: "system", content: "You are a professional TikTok script writer." },
                { role: "user", content: prompt }
            ],
            temperature: 0.7
        })
    });

    if (!response.ok) {
        const err = await response.json();
        throw new Error(`OpenAI Error: ${err.error?.message || response.statusText}`);
    }

    const json = await response.json();
    return json.choices[0].message.content || "";
};

/**
 * Generates a viral TikTok script using the selected AI Provider (OpenAI or Gemini)
 */
export const generateVideoScript = async (
    data: ScriptRequest
): Promise<string> => {
    // Check which AI Provider user selected (default: OpenAI)
    const aiProvider = localStorage.getItem("netflow_ai_provider") || "openai";
    console.log(`ü§ñ Using AI Provider: ${aiProvider.toUpperCase()}`);

    // 1. If OpenAI is selected
    if (aiProvider === "openai") {
        const openaiKey = await getApiKey('openai');
        if (openaiKey) {
            try {
                return await generateWithOpenAI(openaiKey, data);
            } catch (error: any) {
                console.error("‚ùå OpenAI Generation failed:", error.message);
                throw new Error(`OpenAI Error: ${error.message}`);
            }
        } else {
            throw new Error("OpenAI API Key ‡πÑ‡∏°‡πà‡∏û‡∏ö! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà Key ‡πÉ‡∏ô Settings");
        }
    }

    // 2. If Gemini is selected
    if (aiProvider === "gemini") {
        const tryGenerate = async (modelName: string) => {
            console.log(`üî∑ Attempting script generation with Gemini model: ${modelName}`);
            const genAI = await getGenAI();
            const model = genAI.getGenerativeModel({ model: modelName });
            const prompt = buildFullPrompt(data);
            const result = await model.generateContent(prompt);
            const response = await result.response;
            return response.text();
        };

        try {
            return await tryGenerate("gemini-2.0-flash");
        } catch (error: any) {
            console.warn("gemini-2.0-flash failed, trying fallback to gemini-pro...", error.message);
            try {
                return await tryGenerate("gemini-pro");
            } catch (fallbackError: any) {
                console.error("‚ùå All Gemini models failed:", fallbackError.message);
                throw new Error(`Gemini Error: ${fallbackError.message}`);
            }
        }
    }

    // Should not reach here, but just in case
    throw new Error("‡πÑ‡∏°‡πà‡∏û‡∏ö AI Provider ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ô Settings");
};

/**
 * Synthesizes text to speech using Google Cloud TTS API (REST)
 * Note: Requires an API Key with Cloud Text-to-Speech API enabled.
 */
export const generateSpeech = async (text: string): Promise<string | null> => {
    try {
        const apiKey = await getApiKey();
        if (!apiKey) return null;

        const url = `https://texttospeech.googleapis.com/v1/text:synthesize?key=${apiKey}`;

        const requestBody = {
            input: { text: text },
            voice: { languageCode: "th-TH", name: "th-TH-Neural2-C" }, // Defaulting to Thai
            audioConfig: { audioEncoding: "MP3" },
        };

        const response = await fetch(url, {
            method: "POST",
            headers: {
                "Content-Type": "application/json",
            },
            body: JSON.stringify(requestBody),
        });

        if (!response.ok) {
            const err = await response.json();
            console.warn("TTS API Error:", err);
            return null;
        }

        const data = await response.json();
        // API returns audioContent as base64 string
        // Convert to playable Blob URL
        if (data.audioContent) {
            return `data:audio/mp3;base64,${data.audioContent}`;
        }
        return null;

    } catch (error) {
        console.error("Error generating speech:", error);
        return null;
    }
};

/**
 * Placeholder for Veo/Vertex AI Video Generation
 * Note: Vertex AI usually requires Service Account (OAuth), not just API Key.
 * For client-side usage, this part is complex and recommended to move to a proper backend proxy.
 */
export const generateVideo = async (script: string): Promise<string | null> => {
    // TODO: Implement Vertex AI call via Backend Proxy
    console.log("Video generation requires backend implementation via Vertex AI");
    return null;
};

// Orchestrator function to run the full flow
export const runFullWorkflow = async (data: ScriptRequest | AdvancedVideoRequest): Promise<ScriptResult> => {
    console.log("Starting Workflow...", data);

    // SIMULATION MODE REMOVED - Always use real API

    try {
        let script = "";
        let audioUrl: string | undefined = undefined;
        let videoUrl: string | undefined = undefined;

        // 1. Script & Audio Generation (Common Step)
        if ('productName' in data) {
            script = await generateVideoScript(data as ScriptRequest);
            console.log("Script generated:", script);
            audioUrl = (await generateSpeech(script)) || undefined;
        }

        // 2. Advanced Video Pipeline (Nano Banana + Veo + Stitching)
        if ('userImage' in data && (data as AdvancedVideoRequest).userImage) {
            const advData = data as AdvancedVideoRequest;
            console.log("Entering Advanced Video Pipeline...");

            // Step 2.1: Nano Banana Image Generation
            // If user provided an image, we use it as reference to generate a styled image.
            // If prompt is provided, we use it.
            const generatedImage = await generateNanoImage(advData.prompt, advData.userImage);

            if (generatedImage) {
                console.log("Nano Image Generated:", generatedImage);
                // Step 2.2: Loop Video Generation (Real Mode: Veo 3.0)
                const clipUrls: string[] = [];
                const loopCount = advData.loopCount || 1;

                // Helper to generate a single clip with Veo 3.0 LRO
                const generateVeoClip = async (index: number) => {
                    console.log(`[Veo] Starting generation for Clip ${index + 1}...`);
                    try {
                        // 1. Start Long Running Operation
                        const apiKey = await getApiKey();
                        const modelName = "veo-3.0-generate-001";
                        const startUrl = `https://generativelanguage.googleapis.com/v1beta/models/${modelName}:predictLongRunning?key=${apiKey}`;

                        // Clean base64 for API (remove header)
                        const b64Data = generatedImage.split(',')[1] || generatedImage;

                        const startResp = await fetch(startUrl, {
                            method: "POST",
                            headers: { "Content-Type": "application/json" },
                            body: JSON.stringify({
                                instances: [{
                                    // Combine prompt with script context if possible, or just use the visual prompt
                                    prompt: `${advData.prompt} (Clip ${index + 1})`,
                                    image: {
                                        bytesBase64Encoded: b64Data
                                    }
                                }]
                            })
                        });

                        if (!startResp.ok) throw new Error(`Veo Start Failed: ${startResp.statusText}`);
                        const opData = await startResp.json();
                        const opName = opData.name; // "operations/..."
                        console.log(`[Veo] Operation started: ${opName}. Polling...`);

                        // 2. Poll until done
                        let attempts = 0;
                        while (attempts < 60) { // Timeout ~5 mins
                            await new Promise(r => setTimeout(r, 5000)); // Poll every 5s
                            attempts++;

                            console.log(`[Veo] Polling Clip ${index + 1}... attempt ${attempts}`);
                            const currentKey = await getApiKey();
                            const pollUrl = `https://generativelanguage.googleapis.com/v1beta/${opName}?key=${currentKey}`;
                            const pollResp = await fetch(pollUrl);
                            const pollData = await pollResp.json();

                            console.log(`[Veo] Polling Clip ${index + 1}... status: ${pollData.done ? 'DONE' : 'PROCESSING'}`);

                            if (pollData.done) {
                                if (pollData.error) throw new Error(`Veo Operation Error: ${JSON.stringify(pollData.error)}`);

                                // Extract video URL
                                // Result structure depends on API version, usually usually found in 'response'
                                // We'll log it to be sure on first run
                                console.log("[Veo] Operation Result Data:", pollData.response);

                                // Try to find the Video URI (might be GCS or File API)
                                // Standard guess: pollData.response.videoUri or pollData.metadata...
                                // Since we don't have exact schema, let's look for any 'uri' string or base64
                                // If undefined, fallback to mock to not break flow during this critical demo
                                const resultUri = pollData.response?.videoUri || pollData.response?.result?.videoUri;

                                if (resultUri) return resultUri;

                                // Specific fallback for "Demo Effect" if real API returns weird structure
                                // But we return null if we really can't find it
                                console.warn("[Veo] Finished but could not parse Video URI from response.");
                                return null;
                            }
                        }
                        throw new Error("Veo Generation Timed Out");

                    } catch (e: any) {
                        console.error(`[Veo] Error generating Clip ${index + 1}:`, e);
                        // Fallback to mock if individual clip fails so we still get *something*
                        return "https://storage.googleapis.com/gtv-videos-bucket/sample/ForBiggerBlazes.mp4";
                    }
                };

                // Generate clips in parallel or serial? 
                // Veo is expensive/intensive, better do serial to avoid Rate Limits
                for (let i = 0; i < loopCount; i++) {
                    const videoUrl = await generateVeoClip(i);
                    if (videoUrl) clipUrls.push(videoUrl);
                }

                // Step 2.3: Stitching
                if (clipUrls.length > 1 && advData.concatenate) {
                    console.log("Stitching videos...", clipUrls.length);
                    const stitchedUrl = await stitchVideos(clipUrls);
                    if (stitchedUrl) videoUrl = stitchedUrl;
                } else if (clipUrls.length > 0) {
                    videoUrl = clipUrls[0];
                }
            }
        }

        return {
            script,
            audioUrl,
            videoUrl
        };

    } catch (e) {
        console.error("Workflow Failed:", e);
        throw e;
    }
}
