import { GoogleGenerativeAI } from "@google/generative-ai";
import { generateNanoImage } from "./imageGenService";
import { stitchVideos } from "./videoProcessingService";
import { AdvancedVideoRequest, ScriptRequest } from "../types/netflow";
import { getApiKey } from "./storageService";

// Wrapper to get client instance dynamically
const getGenAI = async () => {
    const key = await getApiKey();
    if (!key) throw new Error("API Key not found. Please set it in Settings.");
    return new GoogleGenerativeAI(key);
};

// Interface for the result
export interface ScriptResult {
    script: string;
    audioUrl?: string;
    videoUrl?: string;
    generatedPrompt?: string; // For debugging
    imageUrl?: string; // Fallback image from DALL-E
}

/**
 * Build a comprehensive prompt from all form fields
 */
const buildFullPrompt = (data: ScriptRequest): string => {
    const templateDescriptions: Record<string, string> = {
        "product-review": "‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÅ‡∏ö‡∏ö‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏à (Honest Review) ‡πÄ‡∏ô‡πâ‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÑ‡∏°‡πà‡∏Ç‡∏≤‡∏¢‡∏ù‡∏±‡∏ô",
        "brainrot-product": "‡∏™‡πÑ‡∏ï‡∏•‡πå Brainrot: ‡πÉ‡∏ä‡πâ‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ß‡∏±‡∏¢‡∏£‡∏∏‡πà‡∏ô (Gen Z Slang), ‡∏ï‡∏±‡∏î‡πÑ‡∏ß, ‡∏Å‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó, ‡∏ï‡∏•‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡∏¢, ‡πÑ‡∏°‡πà‡πÄ‡∏ô‡πâ‡∏ô‡∏™‡∏≤‡∏£‡∏∞‡πÅ‡∏ï‡πà‡πÄ‡∏ô‡πâ‡∏ô‡∏Æ‡∏≤",
        "unboxing": "‡πÅ‡∏Å‡∏∞‡∏Å‡∏•‡πà‡∏≠‡∏á‡πÇ‡∏ä‡∏ß‡πå‡∏Ç‡∏≠‡∏á: ‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô‡∏Å‡∏±‡∏ö Packaging, ‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡πÅ‡∏£‡∏Å, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏ï‡∏≠‡∏ô‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡∏≠‡∏á",
        "comparison": "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô: ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠ ‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå‡∏≠‡∏∑‡πà‡∏ô (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏≠‡πà‡∏¢‡∏ä‡∏∑‡πà‡∏≠) ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏á",
        "testimonial": "‡πÄ‡∏•‡πà‡∏≤‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á: ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏°‡∏≤‡πÄ‡∏•‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÑ‡∏á",
        "flash-sale": "Flash Sale: ‡πÄ‡∏£‡πà‡∏á‡∏£‡∏µ‡∏ö, ‡∏î‡∏∏‡∏î‡∏±‡∏ô, ‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏°‡∏î‡πÄ‡∏•‡∏¢, ‡∏ï‡πâ‡∏≠‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏ô‡∏µ‡πâ",
        "tutorial": "How-to: ‡∏™‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡πÅ‡∏ö‡∏ö Step-by-step, ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢, ‡∏ó‡∏≥‡∏ï‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢",
        "lifestyle": "Vlog/Lifestyle: ‡∏ñ‡πà‡∏≤‡∏¢‡∏ó‡∏≠‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏ô‡∏µ‡∏¢‡∏ô‡πÜ",
        "trending": "‡πÄ‡∏Å‡∏≤‡∏∞‡∏Å‡∏£‡∏∞‡πÅ‡∏™: ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏•‡∏á‡∏Æ‡∏¥‡∏ï ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏∏‡∏Å‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏±‡∏á‡πÉ‡∏ô TikTok ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ",
        "mini-drama": "‡∏•‡∏∞‡∏Ñ‡∏£‡∏™‡∏±‡πâ‡∏ô: ‡∏°‡∏µ‡∏û‡∏•‡πá‡∏≠‡∏ï‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏±‡∏Å‡∏°‡∏∏‡∏°, ‡∏î‡∏£‡∏≤‡∏°‡πà‡∏≤, ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏•‡∏Å‡∏Ñ‡∏≤‡πÄ‡∏ü‡πà",
        "before-after": "‡πÇ‡∏ä‡∏ß‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Before/After: ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"
    };

    const toneDescriptions: Record<string, string> = {
        "energetic": "High Energy: ‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô, ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏î‡∏±‡∏á‡∏ü‡∏±‡∏á‡∏ä‡∏±‡∏î, ‡∏Å‡∏£‡∏∞‡∏ï‡∏∑‡∏≠‡∏£‡∏∑‡∏≠‡∏£‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Ç‡∏µ‡∏î (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏û‡∏¥‡∏ò‡∏µ‡∏Å‡∏£‡∏ó‡∏µ‡∏ß‡∏µ‡πÑ‡∏î‡πÄ‡∏£‡πá‡∏Ñ)",
        "calm": "ASMR/Calm: ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ô‡∏∏‡πà‡∏°, ‡∏ó‡∏∏‡πâ‡∏°, ‡∏™‡∏ö‡∏≤‡∏¢‡∏´‡∏π, ‡∏ä‡∏ß‡∏ô‡∏á‡πà‡∏ß‡∏á‡πÅ‡∏ö‡∏ö‡∏ú‡πà‡∏≠‡∏ô‡∏Ñ‡∏•‡∏≤‡∏¢",
        "friendly": "Best Friend: ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏™‡∏ô‡∏¥‡∏ó‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ô, ‡πÉ‡∏ä‡πâ‡∏Å‡∏π-‡∏°‡∏∂‡∏á‡πÑ‡∏î‡πâ (‡∏ñ‡πâ‡∏≤‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°), ‡∏´‡∏£‡∏∑‡∏≠ ‡πÄ‡∏ò‡∏≠-‡∏â‡∏±‡∏ô",
        "professional": "Expert/Professional: ‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠, ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ô‡πà‡∏ô, ‡∏î‡∏π‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç"
    };

    const styleDescriptions: Record<string, string> = {
        "hard": "Hard Sell: ‡∏Ç‡∏≤‡∏¢‡∏ï‡∏£‡∏á‡πÜ ‡πÑ‡∏°‡πà‡∏≠‡πâ‡∏≠‡∏°‡∏Ñ‡πâ‡∏≠‡∏° ‡πÄ‡∏ô‡πâ‡∏ô‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤",
        "soft": "Soft Sell: ‡∏õ‡πâ‡∏≤‡∏¢‡∏¢‡∏≤‡πÅ‡∏ö‡∏ö‡πÄ‡∏ô‡∏µ‡∏¢‡∏ô‡πÜ ‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏ï‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ï‡∏≠‡∏ô‡∏à‡∏ö",
        "educational": "Educational: ‡πÄ‡∏ô‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ ‡∏™‡∏≤‡∏£‡∏∞‡πÅ‡∏ô‡πà‡∏ô‡πÜ ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏Ñ‡πà‡∏ï‡∏±‡∏ß‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö",
        "storytelling": "Storytelling: ‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡∏à‡∏∏‡∏î‡∏û‡∏µ‡∏Ñ ‡πÅ‡∏•‡∏∞‡∏à‡∏∏‡∏î‡∏à‡∏ö (‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏Æ‡∏µ‡πÇ‡∏£‡πà)"
    };

    let prompt = `
You are an expert TikTok/Reels script writer representing a world-class creative agency.
Your goal is to write a script that feels "Premium", "Authentic", and "High-Class".

## CRITICAL INSTRUCTIONS (MUST FOLLOW)
1. **Language Style**: Use **"Spoken Thai" (‡∏†‡∏≤‡∏©‡∏≤‡∏û‡∏π‡∏î)**. Avoid formal/academic Thai (‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô).
   - BAD: "‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç", "‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô", "‡∏à‡∏≥‡∏´‡∏ô‡πà‡∏≤‡∏¢"
   - GOOD: "‡∏ü‡∏¥‡∏ô", "‡∏Å‡∏¥‡∏ô", "‡∏Ç‡∏≤‡∏¢", "‡∏à‡∏∂‡πâ‡∏á‡∏°‡∏≤‡∏Å", "‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ"
   - Use slang naturally (e.g., Fin, Pang, Jeung) to sound viral and relatable.

2. **Visual Description**: When describing scenarios (Proof), focus on **EMOTION & RESULT**.
   - BAD: "In this image..."
   - GOOD: "Look at that face! Pure joy!"

3. **Product Name**: 
   - Use the **EXACT Brand Name** (e.g. "Dior Sauvage", "Whiskas") derived from the input.
   - **MANDATORY**: You MUST mention the Brand Name in the **[HOOK]** and again in the **[CTA]**.
   - Do NOT just say "this perfume" or "this food". Say the NAME.

4. **Structure**: 
   - **HOOk**: Start with a scroll-stopping question or statement + BRAND NAME.
   - **BODY**: Focus on the feeling/benefit.
   - **CLOSING**: Strong Call to Action + BRAND NAME.

## PRODUCT INFORMATION
- Product Name: ${data.productName || "(Extract name from description below)"}
${data.productDescription ? `- Description / Visual Context from AI Brain: ${data.productDescription}` : ""}
${data.mustUseKeywords ? `- Must Include Keywords: ${data.mustUseKeywords}` : ""}
${data.avoidKeywords ? `- Avoid These Words: ${data.avoidKeywords}` : ""}

## SCRIPT SETTINGS
- Template: ${data.template || "product-review"} (${templateDescriptions[data.template || "product-review"] || ""})
- Sales Approach: ${data.style} (${styleDescriptions[data.style] || ""})
- Voice Tone: ${data.tone} (${toneDescriptions[data.tone] || ""})
- Language: ${data.language === "th-central" ? "Thai (Central) - Use modern, natural Thai slang where appropriate" : "English"}

## OUTPUT REQUIREMENTS
Generate a complete TikTok script with:
1. [HOOK]
2. [PROBLEM]
3. [SOLUTION]
4. [PROOF]
5. [CTA]

Output ONLY the script dialogue. No metadata.
`;

    return prompt;
};

/**
 * Uses GPT-4o Vision to analyze the product image (and optional character) to generate a highly detailed prompt
 */
const generateVisualPrompt = async (apiKey: string, imageBase64: string, productName: string, style: string, characterImage?: string): Promise<string> => {
    console.log("üëÅÔ∏è Analyzing Product (and Character) with GPT-4o Vision...");

    try {
        const messagesContent: any[] = [
            {
                type: "text",
                text: `Analyze these images. 
                Image 1: The Product.
                ${characterImage ? "Image 2: The Character/Presenter." : ""}
                
                1. Identify the EXACT Product Name / Brand from the text on the package (if any).
                2. Create a HIGHLY DETAILED, CINEMATIC video generation prompt for a single **8-second continuous shot**.
                   ${characterImage ? "- **INTEGRATION**: Describe the Character (Image 2) interacting with the Product (Image 1) naturally." : ""}
                   - Focus on **CAMERA MOVEMENT** (e.g., "Slow smooth pan", "Rack focus", "Dolly in").
                   - Describe **LIGHTING & ATMOSPHERE** (e.g., "Golden hour", "Neon rim light", "Soft diffusion").
                   - Describe **ACTION** (e.g., "Smoke swirling", "Water droplets falling", "Fabric flowing").
                   - **DO NOT** use "cuts" or "scenes". Write one fluid visual description that lasts 8 seconds.
                   - Style: ${style}.
                
                Output format strictly:
                Name: [Identified Name or "Unknown"]
                Prompt: [Visual Description... (in English)]`
            },
            {
                type: "image_url",
                image_url: { url: imageBase64 }
            }
        ];

        if (characterImage) {
            messagesContent.push({
                type: "image_url",
                image_url: { url: characterImage }
            });
        }

        const response = await fetch("https://api.openai.com/v1/chat/completions", {
            method: "POST",
            headers: {
                "Content-Type": "application/json",
                "Authorization": `Bearer ${apiKey}`
            },
            body: JSON.stringify({
                model: "gpt-4o",
                messages: [
                    {
                        role: "user",
                        content: messagesContent
                    }
                ],
                max_tokens: 350
            })
        });

        const json = await response.json();
        if (json.error || !json.choices) {
            console.warn("Vision API Error:", json.error);
            return `Name: Unknown\nPrompt: Cinematic shot of ${productName || "product"}, ${style} style, professional lighting`;
        }
        return json.choices[0].message.content;

    } catch (e) {
        console.error("Vision Analysis Failed:", e);
        return `Name: Unknown\nPrompt: Cinematic shot of ${productName || "product"}, ${style} style, professional lighting`;
    }
};

/**
 * Helper to generate using OpenAI with full form data (GPT-4o)
 */
const generateWithOpenAI = async (apiKey: string, data: ScriptRequest): Promise<ScriptResult> => {
    console.log("ü§ñ Generating script with OpenAI (GPT-4o)...");
    const prompt = buildFullPrompt(data);

    const response = await fetch("https://api.openai.com/v1/chat/completions", {
        method: "POST",
        headers: {
            "Content-Type": "application/json",
            "Authorization": `Bearer ${apiKey}`
        },
        body: JSON.stringify({
            model: "gpt-4o",
            messages: [
                { role: "system", content: "You are a professional TikTok script writer." },
                { role: "user", content: prompt }
            ],
            temperature: 0.8
        })
    });

    if (!response.ok) {
        const err = await response.json();
        throw new Error(`OpenAI Error: ${err.error?.message || response.statusText}`);
    }

    const json = await response.json();
    return {
        script: json.choices[0].message.content || "",
        generatedPrompt: prompt
    };
};

/**
 * Generates a viral TikTok script using the selected AI Provider (OpenAI or Gemini)
 */
export const generateVideoScript = async (
    data: ScriptRequest
): Promise<ScriptResult> => {
    // Check which AI Provider user selected (default: OpenAI)
    const aiProvider = localStorage.getItem("netflow_ai_provider") || "openai";
    console.log(`ü§ñ Using AI Provider: ${aiProvider.toUpperCase()}`);

    // 1. If OpenAI is selected
    if (aiProvider === "openai") {
        const openaiKey = await getApiKey('openai');
        if (openaiKey) {
            try {
                return await generateWithOpenAI(openaiKey, data);
            } catch (error: any) {
                console.error("‚ùå OpenAI Generation failed:", error.message);
                throw new Error(`OpenAI Error: ${error.message}`);
            }
        } else {
            // If key not found, fallback to Gemini?? 
            // User wants BEST. If no key, maybe Error is better than mediocre fallback. 
            // But existing behavior was Error.
            throw new Error("OpenAI API Key ‡πÑ‡∏°‡πà‡∏û‡∏ö! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà Key ‡πÉ‡∏ô Settings");
        }
    }

    // 2. If Gemini is selected
    if (aiProvider === "gemini") {
        const tryGenerate = async (modelName: string): Promise<ScriptResult> => {
            console.log(`üî∑ Attempting script generation with Gemini model: ${modelName}`);
            const genAI = await getGenAI();
            const model = genAI.getGenerativeModel({ model: modelName });
            const prompt = buildFullPrompt(data);
            const result = await model.generateContent(prompt);
            const response = await result.response;
            return {
                script: response.text(),
                generatedPrompt: prompt
            };
        };

        try {
            return await tryGenerate("gemini-1.5-flash"); // Use 1.5 Flash (latest fast)
        } catch (error: any) {
            console.warn("Gemini Flash failed, trying Pro...", error.message);
            try {
                return await tryGenerate("gemini-pro");
            } catch (fallbackError: any) {
                console.error("‚ùå All Gemini models failed:", fallbackError.message);
                throw new Error(`Gemini Error: ${fallbackError.message}`);
            }
        }
    }

    throw new Error("‡πÑ‡∏°‡πà‡∏û‡∏ö AI Provider ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á");
};

/**
 * Synthesizes text to speech using OpenAI TTS API
 */
const generateOpenAISpeech = async (text: string, apiKey: string): Promise<string | null> => {
    try {
        console.log("üéôÔ∏è Generating speech with OpenAI TTS...");
        const response = await fetch("https://api.openai.com/v1/audio/speech", {
            method: "POST",
            headers: {
                "Content-Type": "application/json",
                "Authorization": `Bearer ${apiKey}`
            },
            body: JSON.stringify({
                model: "tts-1",
                input: text,
                voice: "nova" // "shimmer", "alloy", "echo", "fable", "onyx", "nova"
            })
        });

        if (!response.ok) {
            const err = await response.json();
            console.warn("OpenAI TTS Error:", err);
            return null;
        }

        const blob = await response.blob();
        return new Promise((resolve) => {
            const reader = new FileReader();
            reader.onloadend = () => resolve(reader.result as string);
            reader.readAsDataURL(blob);
        });
    } catch (e) {
        console.error("OpenAI TTS Failed:", e);
        return null;
    }
};

/**
 * Synthesizes text to speech using Google Cloud TTS API (with OpenAI fallback)
 */
export const generateSpeech = async (text: string): Promise<string | null> => {
    // 1. Try Google Cloud TTS first
    try {
        const googleKey = await getApiKey('gemini');
        if (googleKey) {
            const url = `https://texttospeech.googleapis.com/v1/text:synthesize?key=${googleKey}`;
            const requestBody = {
                input: { text },
                voice: { languageCode: "th-TH", ssmlGender: "FEMALE" },
                audioConfig: { audioEncoding: "MP3" },
            };

            const response = await fetch(url, {
                method: "POST",
                headers: { "Content-Type": "application/json" },
                body: JSON.stringify(requestBody),
            });

            if (response.ok) {
                const data = await response.json();
                if (data.audioContent) {
                    console.log("‚úÖ Google TTS Success");
                    return `data:audio/mp3;base64,${data.audioContent}`;
                }
            } else {
                console.warn("Google TTS failed, trying OpenAI fallback...");
            }
        }
    } catch (error) {
        console.warn("Google TTS error, trying OpenAI fallback...");
    }

    // 2. Fallback to OpenAI TTS
    const openaiKey = await getApiKey('openai');
    if (openaiKey) {
        return await generateOpenAISpeech(text, openaiKey);
    }

    return null;
};

/**
 * Placeholder for Veo/Vertex AI Video Generation
 * Note: Vertex AI usually requires Service Account (OAuth), not just API Key.
 * For client-side usage, this part is complex and recommended to move to a proper backend proxy.
 */
export const generateVideo = async (script: string): Promise<string | null> => {
    // TODO: Implement Vertex AI call via Backend Proxy
    console.log("Video generation requires backend implementation via Vertex AI");
    return null;
};

/**
 * RPA Binding: Triggers the Service Worker to run Google VideoFX automation
 */
const triggerRPAGeneration = (prompt: string): Promise<string> => {
    return new Promise((resolve, reject) => {
        // Check if running as extension
        if (typeof chrome === "undefined" || !chrome.runtime) {
            console.warn("RPA requires Chrome Extension environment. Returning mock.");
            setTimeout(() => resolve("https://storage.googleapis.com/gtv-videos-bucket/sample/ForBiggerBlazes.mp4"), 3000);
            return;
        }

        console.log("üöÄ Triggering RPA Flow with prompt:", prompt);

        // 1. Send Start Command
        chrome.runtime.sendMessage({
            type: "START_VIDEO_GENERATION",
            payload: { prompt }
        }, (response) => {
            if (chrome.runtime.lastError) {
                reject(new Error("Extension Error: " + chrome.runtime.lastError.message));
                return;
            }
            console.log("RPA Started:", response);
        });

        // 2. Listen for Completion (One-time listener)
        const listener = (message: any) => {
            if (message.type === "VIDEO_GENERATION_COMPLETE") {
                console.log("‚úÖ RPA Video Received:", message.videoUrl);
                cleanup();
                resolve(message.videoUrl);
            } else if (message.type === "VIDEO_GENERATION_ERROR") {
                console.error("‚ùå RPA Error:", message.error);
                cleanup();
                reject(new Error(message.error));
            }
        };

        // Cleanup listener to avoid leaks
        const cleanup = () => {
            chrome.runtime.onMessage.removeListener(listener);
        };

        chrome.runtime.onMessage.addListener(listener);

        // Timeout (5 minutes max)
        setTimeout(() => {
            cleanup();
            reject(new Error("RPA Timeout (5 mins)"));
        }, 5 * 60 * 1000);
    });
};

// Orchestrator function to run the full flow
export const runFullWorkflow = async (data: ScriptRequest | AdvancedVideoRequest): Promise<ScriptResult> => {
    console.log("Starting Workflow...", data);

    try {
        let script = "";
        let audioUrl: string | undefined = undefined;
        let videoUrl: string | undefined = undefined;
        let imageUrl: string | undefined = undefined;
        let generatedPrompt: string | undefined = undefined;

        // Cast to AdvancedVideoRequest safely
        let advData = data as any;

        // 1. Vision Analysis (Brain üß†)
        if (advData.userImage) {
            console.log("üß† Starting Smart Vision Analysis (GPT-4o Vision)...");
            // Note: In real extension, we might need a persistent key or proxy. 
            // Here we reuse getApiKey('openai') logic.
            const apiKey = await getApiKey('openai');
            if (apiKey) {
                const visionRes = await generateVisualPrompt(apiKey, advData.userImage, advData.productName, advData.style, advData.characterImage);
                if (visionRes) {
                    // Parse logic: "Name: [xxx]\nPrompt: [yyy]"
                    const nameMatch = visionRes.match(/Name:\s*(.+)/i);
                    const promptMatch = visionRes.match(/Prompt:\s*([\s\S]+)/i);

                    const extractedName = nameMatch ? nameMatch[1].trim() : "Unknown";
                    const extractedPrompt = promptMatch ? promptMatch[1].trim() : visionRes; // Fallback to whole text

                    console.log(`‚úÖ Smart Vision found: Name=[${extractedName}]`);

                    // Update Data
                    if (extractedName && extractedName !== "Unknown" && (!advData.productName || advData.productName === "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ")) {
                        advData.productName = extractedName;
                    }
                    advData.prompt = extractedPrompt;

                    // Inject into Product Description for Script Writer (Context)
                    if (!advData.productDescription) advData.productDescription = "";
                    advData.productDescription += `\n\n[Visual Details]: ${extractedPrompt}`;

                    generatedPrompt = `[Smart Vision]: ${visionRes}`;
                }
            }
        }

        let visualPrompt = advData.prompt || advData.productName;

        // 2. Script & Audio Generation (Brain üß†)
        // Run if we have a name OR image OR minimal context
        if (advData.productName || advData.userImage || advData.productDescription) {
            console.log("üìù Generating Script for:", advData.productName || "Unknown Product");
            const result = await generateVideoScript(advData);
            script = result.script;
            if (!generatedPrompt) generatedPrompt = result.generatedPrompt;
            else generatedPrompt += `\n\n[Script Prompt]: ${result.generatedPrompt}`;

            console.log("Script generated:", script);
            audioUrl = (await generateSpeech(script)) || undefined;
        }

        // 3. Visual Generation (RPA Automaton ü§ñ)
        // Only if userImage or Prompt is present
        if (visualPrompt) {
            console.log("üé• Starting Indirect Video Generation (RPA)...");

            try {
                // Trigger the hidden "Ghost Browser" to create video
                videoUrl = await triggerRPAGeneration(visualPrompt);

                if (videoUrl) {
                    console.log("üéâ Video Generated via RPA!");
                }
            } catch (rpaError) {
                console.error("RPA Generation Failed:", rpaError);
                // No fallback to DALL-E (Indirect policy)
            }
        }

        return {
            script,
            audioUrl,
            videoUrl,
            imageUrl,
            generatedPrompt
        };

    } catch (e) {
        console.error("Workflow Failed:", e);
        throw e;
    }
};
